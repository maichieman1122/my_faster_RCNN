{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7fe63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  absolute_import\n",
    "from __future__ import  division\n",
    "import torch as t\n",
    "from data.voc_dataset import VOCBboxDataset\n",
    "from skimage import transform as sktsf\n",
    "from torchvision import transforms as tvtsf\n",
    "from data import util\n",
    "import numpy as np\n",
    "from our_utils.config import opt\n",
    "\n",
    "def pytorch_normalze(img):\n",
    "    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()\n",
    "\n",
    "def preprocess(img, min_size=1024, max_size=2048):\n",
    "    C, H, W=img.shape\n",
    "    scale1 =min_size / min(H, W)\n",
    "    scale2 =max_size / max(H, W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img/255.\n",
    "    img=sktsf.resize(img, (C, H*scale, W*scale), mode='reflect', anti_aliasing=False)\n",
    "    \n",
    "    normalize=pytorch_normalze\n",
    "    return normalize(img)\n",
    "\n",
    "class Transform(object):\n",
    "    def __init__(self, min_size=1024, max_size=2048):\n",
    "        self.min_size=min_size\n",
    "        self.max_size=max_size\n",
    "    def __call__(self, in_data):\n",
    "        img, bbox, label=in_data\n",
    "        _,H,W=img.shape\n",
    "        img=preprocess(img, self.min_size, self.max_size)\n",
    "        _, o_H, o_W=img.shape\n",
    "        scale=o_H / H\n",
    "        bbox=util.resize_bbox(bbox,(H,W),(o_H, o_W))\n",
    "        \n",
    "        #horizontally flip\n",
    "        img, params=util.random_flip(img, x_random=True, return_param=True)\n",
    "        bbox=util.flip_bbox(bbox,(o_H, o_W), x_flip=params['x_flip'])\n",
    "        return img, bbox, label, scale\n",
    "class Dataset:\n",
    "    def __init__(self, opt):\n",
    "        self.opt=opt\n",
    "        self.db=VOCBboxDataset(opt.voc_data_dir)\n",
    "        self.tsf=Transform(opt.min_size, opt.max_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ori_img, bbox, label, difficult= self.db[idx]\n",
    "        img, bbox, label, scale=self.tsf((ori_img, bbox, label))\n",
    "        return img.copy(), bbox.copy(), label.copy(), scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.db.__len__())\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, opt, split='test',use_difficult=True):\n",
    "        self.opt=opt\n",
    "        self.db=VOCBboxDataset(opt.voc_data_dir, split=split, use_difficult=use_difficult)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ori_img, bbox, label, difficult=self.db[idx]\n",
    "        img=preprocess(ori_img)\n",
    "        return img, ori_img.shape[1:], bbox, label, difficult\n",
    "    def __len__(self):\n",
    "        return int(self.db.__len__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
